---
title: k-means cross-validation demo
author: Peter Carbonetto
output: workflowr::wflow_html
---

This is another simple illustration of the `perform_cv`
cross-validation interface in which we use cross-validation to select
the $k$ (the number of clusters) in $k$-means. This example introduces
two slight complications that didn't arise in the Elastic Net demo:

1. The $k$-means output depends on initialization. We address this by
   providing a common initialization for all the $k$-means runs. This
   ensures that `perform_cv` always produces the same result.

2. The $k$-means problem is an *unsupervised learning problem*, so $Y$
   (which we define to be the unknown cluster centers) is not used in
   the "fit" function, and is only used for evaluating the quality of
   the fit.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the `perform_cv` code.

```{r load-pkgs, message=FALSE}
library(glmnet)
library(parallel)
source("../code/cv.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

Simulate a clustering data set.

```{r sim-data, fig.height=3, fig.width=3.25}
n <- 400
k <- 5
centers <- matrix(rnorm(2*k),k,2)
membership <- sample(k,n,replace = TRUE)
X <- matrix(0,n,2)
for (i in 1:n) {
  j     <- membership[i]
  X[i,] <- centers[j,] + rnorm(2)/4
}
par(mar = c(4,4,0,0))
plot(X[,1],X[,2],col = "royalblue",pch = 1,cex = 0.75,xlab = "x1",ylab = "x2")
points(centers[,1],centers[,2],col = "red",pch = 20,cex = 1)
```

The solid red points are the cluster centers.

Now run *k*-means once with $k = 10$ clusters. We will use this to
initialize the other runs of *k*-means.

```{r kmeans-k-10}
fit_k10 <- kmeans(X,centers = 10,iter.max = 100)
```

This function runs *k*-means, initializing the cluster centers using
the *k*-means clustering result with $k = 10$ clusters.

```{r run-kmeans}
run_kmeans <- function (x, y, cvpar)
  kmeans(x,fit_k10$centers[1:cvpar,],iter.max = 100)
```
  
This function assigns the "best-fit" cluster centers to the data
points.

```{r predict-kmeans}
predict_kmeans <- function (x, model) {
  k <- nrow(model$centers)
  D <- as.matrix(dist(rbind(model$centers,x)))
  D <- D[1:k,-(1:k)]
  i <- apply(D,2,which.min)
  return(model$centers[i,])
}
```

This function computes the mean squared error (MSE) between the
estimated cluster centers and the true cluster centers.

```{r compute-mse}
compute_mse <- function (pred, true)
  mean((pred - true)^2)
```

Having defined these three functions, and determined a common
initialization for all the *k*-means runs, we are now ready to use
`perform_cv`.

```{r run-cv}
k <- 2:10
cv <- perform_cv(run_kmeans,predict_kmeans,compute_mse,X,
                 centers[membership,],k)
```

Now let's see how the prediction error evolves as we change $k$.

```{r plot-k-vs-mse, fig.height=2.8, fig.width=3}
par(mar = c(4,4,0,0))
plot(k,rowMeans(cv),type = "l",col = "darkblue",lwd = 2,
     xlab = "k",ylab = "mse")
points(k,rowMeans(cv),pch = 20)
```

Reassuringly, the lowest error is achieved at the correct number of
clusters ($k = 5$).
